{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LED_summarizer_coliee21",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi5yCW0NKInv"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "#!wget < link for led_train_for_testing_on_fold_0.zip?dl=0, email a.askari@liacs.leidenuniv.nl for this checkpoint>\n",
        "!unzip led_train_for_testing_on_fold_0.zip?dl=0\n",
        "!pip install jsonlines"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZtKaWsNfGyL"
      },
      "source": [
        "%%capture\n",
        "from transformers import LEDForConditionalGeneration, LEDTokenizer\n",
        "import torch\n",
        "trained_on_colie18 = \"./led_train_for_testing_on_fold_0\"\n",
        "tokenizer_led = LEDTokenizer.from_pretrained(trained_on_colie18)\n",
        "model_led = LEDForConditionalGeneration.from_pretrained(trained_on_colie18, return_dict_in_generate=True).to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWZA43YwjRn4"
      },
      "source": [
        "import os,sys\n",
        "import random\n",
        "import jsonlines\n",
        "import ast\n",
        "import json\n",
        "import time\n",
        "start = time.time()\n",
        "jsonl_path = \"./whole_doc_w_summ_intro_w_newline_char/separately_para_w_summ_intro_newline_char.jsonl\"\n",
        "cnt = 0\n",
        "do_no_repeat_ngram_size =2 #fix it\n",
        "\n",
        "with open(jsonl_path) as fp:\n",
        "  for line in fp:\n",
        "    cnt += 1\n",
        "    if cnt % 10000 == 0:\n",
        "      print(\"{} lines parsed\".format(cnt))\n",
        "    line_dict = ast.literal_eval(line)\n",
        "    id_ = line_dict[\"id\"]\n",
        "    contents = line_dict[\"contents\"]\n",
        "    line = contents\n",
        "\n",
        "    input_ids = tokenizer_led(line, max_length=8192, return_tensors=\"pt\").input_ids.to(\"cuda\") #cuda\n",
        "    tokens_len = len(input_ids[0])\n",
        "\n",
        "    max_len_summary = int(tokens_len*10/100)\n",
        "    print(\"max_len_summary int(tokens_len*10/100): \", int(tokens_len*10/100))\n",
        "\n",
        "    global_attention_mask = torch.zeros_like(input_ids)\n",
        "    sequences = model_led.generate(input_ids, global_attention_mask=global_attention_mask, max_length=max_len_summary, no_repeat_ngram_size=do_no_repeat_ngram_size).sequences\n",
        "    summary = tokenizer_led.batch_decode(sequences, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    summary_output = ' '.join(summary)\n",
        "    summary_output = summary_output.replace(\"\\n\",\" \")\n",
        "    print(\"\\n----\\n\\ngiven the whole document to the model led arxiv, summary is: \\n\\t\", summary)\n",
        "    print(\"tokens len, \", tokens_len, \"| max len for summary: \", max_len_summary , \"\\n------\\n\\n\\n\")\n",
        "\n",
        "    with open('separately_para_w_summ_intro_summaries.jsonl', 'a') as outfile:\n",
        "        entry = {\"id\": id_, \"contents\":summary_output}\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')\n",
        "    print('It took', time.time()-start, 'seconds.')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}